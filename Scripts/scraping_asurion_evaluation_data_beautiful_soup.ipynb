{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "207787c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02672afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_soup_for_page(page_number):\n",
    "    \"\"\"We initialize a soup object that will contain all the tags that are on the main part of the Trustpilot website for the company Asurion :\n",
    "        https://www.trustpilot.com/review/www.asurion.com\n",
    "        The website contains client ratings on multiple pages.\n",
    "        In the function we can choose for which page number we want to retrieve the data.\n",
    "    -----------------\n",
    "       Parameters\n",
    "    -----------------\n",
    "    page_number : insert an int value between 1 and the biggest existing page number for the company \n",
    "\n",
    "    -----------------\n",
    "        Returns\n",
    "    -----------------\n",
    "    soup object containing the tags of the selected website\n",
    "    \"\"\"\n",
    "    \n",
    "    url = ''\n",
    "    if page_number == 1:\n",
    "        url = \"https://www.trustpilot.com/review/www.asurion.com\"\n",
    "    else:\n",
    "        url = f\"https://www.trustpilot.com/review/www.asurion.com?page={page_number}\"\n",
    "    page = urlopen(url)\n",
    "    soup = bs(page, \"html.parser\")\n",
    "    evaluations = soup.findAll('div', attrs = {'class' : \"styles_cardWrapper__LcCPA styles_show__HUXRb styles_reviewCard__9HxJJ\"})\n",
    "    return evaluations\n",
    "    \n",
    "def insert_row(df, my_row):\n",
    "    \"\"\"Insert a list in an existing DataFrame. The length of the list must be the same as the number of the columns in the DataFrame.\n",
    "    -----------------\n",
    "       Parameters\n",
    "    -----------------\n",
    "    df : the DataFrame in which we want to insert a new list as the last row of the DataFrame \n",
    "    my_row : the list we want to insert into the DataFrame\n",
    "    \n",
    "    -----------------\n",
    "        Returns\n",
    "    -----------------\n",
    "    None\n",
    "    \"\"\"\n",
    "    df.loc[len(df)] = my_row\n",
    "\n",
    "def get_specific_data_for_page(page_number):   \n",
    "    \"\"\" After we get all the data for a specific page with the function initialize_soup_for_page(), we go over each evaluation on the page\n",
    "        and we will select the specific tags (within the evaluation) that we want to keep.\n",
    "        The values of these tags will be stored into a DataFrame.\n",
    "    -----------------\n",
    "       Parameters\n",
    "    -----------------\n",
    "    page_number : this parameter will be used by the initialize_soup_for_page() function and it is the same (it represent the page that we want to scrape) \n",
    "    -----------------\n",
    "        Returns\n",
    "    -----------------\n",
    "    df_evals : return a df containing specific data of each evaluation on a chosen page\n",
    "        The speficifc data we acquire is : \n",
    "            - comment title\n",
    "            - name of person who gives evaluation\n",
    "            - number of stars left by this person\n",
    "            - localisation of this person\n",
    "            - number of reviews left by this person\n",
    "            - date of the evaluation\n",
    "            - the date of experience that the person had\n",
    "            - comment left by the person\n",
    "    \"\"\"\n",
    "        \n",
    "    df_evals = pd.DataFrame(columns = ['titre','nom','stars','localisation','nb_reviews','date_review','date_experience','comment'])\n",
    "    evaluations = initialize_soup_for_page(page_number)\n",
    "    for e in evaluations:\n",
    "        titre_ = e.find('h2', {'class': 'typography_heading-s__f7029 typography_appearance-default__AAY17'})\n",
    "        titre = titre_.text if titre_ is not None else ''\n",
    "        \n",
    "        nom_ = e.find('span', {'class': 'typography_heading-xxs__QKBS8 typography_appearance-default__AAY17'})\n",
    "        nom = nom_.text if nom_ is not None else ''\n",
    "        \n",
    "        stars_ = e.find('img')['alt']\n",
    "        stars = stars_ if stars_ is not None else ''\n",
    "        \n",
    "        localisation_ = e.find('div', {'class': 'typography_body-m__xgxZ_ typography_appearance-subtle__8_H2l styles_detailsIcon__Fo_ua'}).find('span')\n",
    "        localisation = localisation_.text if localisation_ is not None else ''\n",
    "        \n",
    "        nb_reviews_ = e.find('span', {'class': 'typography_body-m__xgxZ_ typography_appearance-subtle__8_H2l'})\n",
    "        nb_reviews = nb_reviews_.text if nb_reviews_ is not None else ''\n",
    "        \n",
    "        date_review_ = e.find('time')['datetime']\n",
    "        date_review = date_review_ if date_review_ is not None else ''\n",
    "        \n",
    "        date_experience_ = e.find('p',{'class':'typography_body-m__xgxZ_ typography_appearance-default__AAY17'})\n",
    "        date_experience = date_experience_.text if date_experience_ is not None else ''\n",
    "        \n",
    "        comment_ = e.find('p', {'class': 'typography_body-l__KUYFJ typography_appearance-default__AAY17 typography_color-black__5LYEn'})\n",
    "        comment = comment_.text if comment_ is not None else ''\n",
    "        \n",
    "        insert_row(df_evals, [titre,nom,stars,localisation,nb_reviews,date_review,date_experience,comment])\n",
    "    return df_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c8b0254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specific_data_for_specific_pages(first_page = 1,last_page = 4590):\n",
    "    \"\"\" WARNING : last_page value cant be bigger than the number of pages that exist\n",
    "        \n",
    "        The function goes over all the pages that are selected in the parameters.\n",
    "        With the get_specific_data_for_page() function we scrape all the chosen data of these pages.\n",
    "        \n",
    "        Error message appears when we scrape too much data in a short time period. So after each of these error messages,\n",
    "        we wait 8 minutes before we start scraping again from where we left off.\n",
    "    -----------------\n",
    "       Parameters\n",
    "    -----------------\n",
    "    first_page = 1 : the page from we which we want to start scraping\n",
    "    last_page = 4590 : the last page that exists and that we want to scrape \n",
    "    -----------------\n",
    "        Returns\n",
    "    -----------------\n",
    "    df_evals : return a df containing concatenated data of each evaluation on all the selected pages\n",
    "    \n",
    "    \"\"\"\n",
    "    df_evals = pd.DataFrame(columns = ['titre','nom','stars','localisation','nb_reviews','date_review','date_experience','comment'])\n",
    "    for n in range(first_page,last_page):\n",
    "        try :\n",
    "            df_evals = pd.concat([df_evals,get_specific_data_for_page(n)])\n",
    "            print(f\"Page {n} was succesfully scraped.\") if n%10 == 0 else None\n",
    "        except:\n",
    "            print(f\"For some reason data scraping couldn't be executed for page {n}.\")\n",
    "            print(\"Maybe the page we selected to scrape doesn't exist.\")\n",
    "            print(\"Or we reached the amount of data we could scrape within in a specific time period.\")\n",
    "            print(\"Let's wait a few minutes and start scraping again.\")\n",
    "            print(\"===\"*20)\n",
    "            print(\"Countdown 8 minutes :\")\n",
    "            print(\"===\"*20)\n",
    "            for i in range(480,0,-1):\n",
    "                time.sleep(1)\n",
    "                sys.stdout.write(str(i)+', ')\n",
    "            df_evals = pd.concat([df_evals, get_specific_data_for_page(n)])\n",
    "            print(f\"Page {n} was succesfully scraped.\")\n",
    "    return df_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef3cb3e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df1 = get_specific_data_for_specific_pages(first_page = 1, last_page = 1000)\n",
    "#df1.to_csv('asurion1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cadffa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = get_specific_data_for_specific_pages(first_page = 1000, last_page = 2000)\n",
    "#df2.to_csv('asurion2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cf13f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3 = get_specific_data_for_specific_pages(first_page = 2000, last_page = 3000)\n",
    "#df3.to_csv('asurion3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c8928fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df4 = get_specific_data_for_specific_pages(first_page = 3000, last_page = 4000)\n",
    "#df4.to_csv('asurion4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c9721a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df5 = get_specific_data_for_specific_pages(first_page = 4000, last_page = 4590) # careful about the value of last_page\n",
    "#df5.to_csv('asurion5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c702641c",
   "metadata": {},
   "source": [
    "# `WARNING`:\n",
    "\n",
    "If we put a higher number for last_page than how many actually exist, the program will run infinitely and nothing will be added to the variable df5.\n",
    "\n",
    "This part should be still handled so the program automatically stops when we reach the last page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66425881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.concat([df1,df2,df3,df4,df5])\\\n",
    "#    .reset_index(drop=True) \\\n",
    "#    .drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8f627a",
   "metadata": {},
   "source": [
    "Scraping all the data was't done all at once so we got the data with 1000 page batches.\n",
    "\n",
    "We import these into a final df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69ac4a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91763, 8)\n"
     ]
    }
   ],
   "source": [
    "# import all files that has the scraped data\n",
    "data_files = ['asurion1.csv', 'asurion2.csv', 'asurion3.csv', 'asurion4.csv', 'asurion5.csv']\n",
    "df = pd.concat((pd.read_csv(filename) for filename in data_files))\n",
    "df = df.drop_duplicates() \\\n",
    "    .reset_index(drop = True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "178fda1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the files into one CSV file\n",
    "df.to_csv('asurion_complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "533bd9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the files into one JSON file\n",
    "df.to_json('asurion_complete.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
