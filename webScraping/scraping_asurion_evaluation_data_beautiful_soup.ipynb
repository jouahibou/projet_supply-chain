{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "207787c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02672afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_soup_for_page(page_number):\n",
    "    \"\"\"We initialize a soup object that will contain all the tags that are on the main part of the Trustpilot website for the company Asurion :\n",
    "        https://www.trustpilot.com/review/www.asurion.com\n",
    "        The website contains client ratings on multiple pages.\n",
    "        In the function we can choose for which page number we want to retrieve the data.\n",
    "    -----------------\n",
    "       Parameters\n",
    "    -----------------\n",
    "    page_number : insert an int value between 1 and the biggest existing page number for the company \n",
    "\n",
    "    -----------------\n",
    "        Returns\n",
    "    -----------------\n",
    "    soup object containing the tags of the selected website\n",
    "    \"\"\"\n",
    "    \n",
    "    url = ''\n",
    "    if page_number == 1:\n",
    "        url = \"https://www.trustpilot.com/review/www.asurion.com\"\n",
    "    else:\n",
    "        url = f\"https://www.trustpilot.com/review/www.asurion.com?page={page_number}\"\n",
    "    page = urlopen(url)\n",
    "    soup = bs(page, \"html.parser\")\n",
    "    evaluations = soup.findAll('div', attrs = {'class' : \"styles_cardWrapper__LcCPA styles_show__HUXRb styles_reviewCard__9HxJJ\"})\n",
    "    return evaluations\n",
    "    \n",
    "def insert_row(df, my_row):\n",
    "    \"\"\"Insert a list in an existing DataFrame. The length of the list must be the same as the number of the columns in the DataFrame.\n",
    "    -----------------\n",
    "       Parameters\n",
    "    -----------------\n",
    "    df : the DataFrame in which we want to insert a new list as the last row of the DataFrame \n",
    "    my_row : the list we want to insert into the DataFrame\n",
    "    \n",
    "    -----------------\n",
    "        Returns\n",
    "    -----------------\n",
    "    None\n",
    "    \"\"\"\n",
    "    df.loc[len(df)] = my_row\n",
    "\n",
    "def get_specific_data_for_page(page_number):   \n",
    "    \"\"\" After we get all the data for a specific page with the function initialize_soup_for_page(), we go over each evaluation on the page\n",
    "        and we will select the specific tags (within the evaluation) that we want to keep.\n",
    "        The values of these tags will be stored into a DataFrame.\n",
    "    -----------------\n",
    "       Parameters\n",
    "    -----------------\n",
    "    page_number : this parameter will be used by the initialize_soup_for_page() function and it is the same (it represent the page that we want to scrape) \n",
    "    -----------------\n",
    "        Returns\n",
    "    -----------------\n",
    "    df_evals : return a df containing specific data of each evaluation on a chosen page\n",
    "        The speficifc data we acquire is : \n",
    "            - comment title\n",
    "            - name of person who gives evaluation\n",
    "            - number of stars left by this person\n",
    "            - localisation of this person\n",
    "            - number of reviews left by this person\n",
    "            - date of the evaluation\n",
    "            - the date of experience that the person had\n",
    "            - comment left by the person\n",
    "    \"\"\"\n",
    "        \n",
    "    df_evals = pd.DataFrame(columns = ['titre','nom','stars','localisation','nb_reviews','date_review','date_experience','comment'])\n",
    "    evaluations = initialize_soup_for_page(page_number)\n",
    "    for e in evaluations:\n",
    "        titre_ = e.find('h2', {'class': 'typography_heading-s__f7029 typography_appearance-default__AAY17'})\n",
    "        titre = titre_.text if titre_ is not None else ''\n",
    "        \n",
    "        nom_ = e.find('span', {'class': 'typography_heading-xxs__QKBS8 typography_appearance-default__AAY17'})\n",
    "        nom = nom_.text if nom_ is not None else ''\n",
    "        \n",
    "        stars_ = e.find('img')['alt']\n",
    "        stars = stars_ if stars_ is not None else ''\n",
    "        \n",
    "        localisation_ = e.find('div', {'class': 'typography_body-m__xgxZ_ typography_appearance-subtle__8_H2l styles_detailsIcon__Fo_ua'}).find('span')\n",
    "        localisation = localisation_.text if localisation_ is not None else ''\n",
    "        \n",
    "        nb_reviews_ = e.find('span', {'class': 'typography_body-m__xgxZ_ typography_appearance-subtle__8_H2l'})\n",
    "        nb_reviews = nb_reviews_.text if nb_reviews_ is not None else ''\n",
    "        \n",
    "        date_review_ = e.find('time')['datetime']\n",
    "        date_review = date_review_ if date_review_ is not None else ''\n",
    "        \n",
    "        date_experience_ = e.find('p',{'class':'typography_body-m__xgxZ_ typography_appearance-default__AAY17'})\n",
    "        date_experience = date_experience_.text if date_experience_ is not None else ''\n",
    "        \n",
    "        comment_ = e.find('p', {'class': 'typography_body-l__KUYFJ typography_appearance-default__AAY17 typography_color-black__5LYEn'})\n",
    "        comment = comment_.text if comment_ is not None else ''\n",
    "        \n",
    "        insert_row(df_evals, [titre,nom,stars,localisation,nb_reviews,date_review,date_experience,comment])\n",
    "    return df_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c8b0254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specific_data_for_specific_pages(first_page = 1,last_page = 4590):\n",
    "    \"\"\" WARNING : last_page value cant be bigger than the number of pages that exist\n",
    "        \n",
    "        The function goes over all the pages that are selected in the parameters.\n",
    "        With the get_specific_data_for_page() function we scrape all the chosen data of these pages.\n",
    "        \n",
    "        Error message appears when we scrape too much data in a short time period. So after each of these error messages,\n",
    "        we wait 8 minutes before we start scraping again from where we left off.\n",
    "    -----------------\n",
    "       Parameters\n",
    "    -----------------\n",
    "    first_page = 1 : the page from we which we want to start scraping\n",
    "    last_page = 4590 : the last page that exists and that we want to scrape \n",
    "    -----------------\n",
    "        Returns\n",
    "    -----------------\n",
    "    df_evals : return a df containing concatenated data of each evaluation on all the selected pages\n",
    "    \n",
    "    \"\"\"\n",
    "    df_evals = pd.DataFrame(columns = ['titre','nom','stars','localisation','nb_reviews','date_review','date_experience','comment'])\n",
    "    for n in range(first_page,last_page):\n",
    "        try :\n",
    "            df_evals = pd.concat([df_evals,get_specific_data_for_page(n)])\n",
    "            print(f\"Page {n} was succesfully scraped.\") if n%10 == 0 else None\n",
    "        except:\n",
    "            print(f\"For some reason data scraping couldn't be executed for page {n}.\")\n",
    "            print(\"Maybe the page we selected to scrape doesn't exist.\")\n",
    "            print(\"Or we reached the amount of data we could scrape within in a specific time period.\")\n",
    "            print(\"Let's wait a few minutes and start scraping again.\")\n",
    "            print(\"===\"*20)\n",
    "            print(\"Countdown 8 minutes :\")\n",
    "            print(\"===\"*20)\n",
    "            for i in range(480,0,-1):\n",
    "                time.sleep(1)\n",
    "                sys.stdout.write(str(i)+', ')\n",
    "            df_evals = pd.concat([df_evals, get_specific_data_for_page(n)])\n",
    "            print(f\"Page {n} was succesfully scraped.\")\n",
    "    return df_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef3cb3e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df1 = get_specific_data_for_specific_pages(first_page = 1, last_page = 1000)\n",
    "#df1.to_csv('asurion1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cadffa84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df2 = get_specific_data_for_specific_pages(first_page = 1000, last_page = 2000)\n",
    "#df2.to_csv('asurion2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cf13f0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df3 = get_specific_data_for_specific_pages(first_page = 2000, last_page = 3000)\n",
    "#df3.to_csv('asurion3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c8928fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df4 = get_specific_data_for_specific_pages(first_page = 3000, last_page = 4000)\n",
    "#df4.to_csv('asurion4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c9721a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 4000 was succesfully scraped.\n",
      "Page 4010 was succesfully scraped.\n",
      "Page 4020 was succesfully scraped.\n",
      "Page 4030 was succesfully scraped.\n",
      "Page 4040 was succesfully scraped.\n",
      "Page 4050 was succesfully scraped.\n",
      "Page 4060 was succesfully scraped.\n",
      "Page 4070 was succesfully scraped.\n",
      "Page 4080 was succesfully scraped.\n",
      "Page 4090 was succesfully scraped.\n",
      "Page 4100 was succesfully scraped.\n",
      "Page 4110 was succesfully scraped.\n",
      "Page 4120 was succesfully scraped.\n",
      "Page 4130 was succesfully scraped.\n",
      "Page 4140 was succesfully scraped.\n",
      "Page 4150 was succesfully scraped.\n",
      "Page 4160 was succesfully scraped.\n",
      "Page 4170 was succesfully scraped.\n",
      "Page 4180 was succesfully scraped.\n",
      "For some reason data scraping couldn't be executed for page 4186.\n",
      "Maybe the page we selected to scrape doesn't exist.\n",
      "Or we reached the amount of data we could scrape within in a specific time period.\n",
      "Let's wait a few minutes and start scraping again.\n",
      "============================================================\n",
      "Countdown 8 minutes :\n",
      "============================================================\n",
      "480, 479, 478, 477, 476, 475, 474, 473, 472, 471, 470, 469, 468, 467, 466, 465, 464, 463, 462, 461, 460, 459, 458, 457, 456, 455, 454, 453, 452, 451, 450, 449, 448, 447, 446, 445, 444, 443, 442, 441, 440, 439, 438, 437, 436, 435, 434, 433, 432, 431, 430, 429, 428, 427, 426, 425, 424, 423, 422, 421, 420, 419, 418, 417, 416, 415, 414, 413, 412, 411, 410, 409, 408, 407, 406, 405, 404, 403, 402, 401, 400, 399, 398, 397, 396, 395, 394, 393, 392, 391, 390, 389, 388, 387, 386, 385, 384, 383, 382, 381, 380, 379, 378, 377, 376, 375, 374, 373, 372, 371, 370, 369, 368, 367, 366, 365, 364, 363, 362, 361, 360, 359, 358, 357, 356, 355, 354, 353, 352, 351, 350, 349, 348, 347, 346, 345, 344, 343, 342, 341, 340, 339, 338, 337, 336, 335, 334, 333, 332, 331, 330, 329, 328, 327, 326, 325, 324, 323, 322, 321, 320, 319, 318, 317, 316, 315, 314, 313, 312, 311, 310, 309, 308, 307, 306, 305, 304, 303, 302, 301, 300, 299, 298, 297, 296, 295, 294, 293, 292, 291, 290, 289, 288, 287, 286, 285, 284, 283, 282, 281, 280, 279, 278, 277, 276, 275, 274, 273, 272, 271, 270, 269, 268, 267, 266, 265, 264, 263, 262, 261, 260, 259, 258, 257, 256, 255, 254, 253, 252, 251, 250, 249, 248, 247, 246, 245, 244, 243, 242, 241, 240, 239, 238, 237, 236, 235, 234, 233, 232, 231, 230, 229, 228, 227, 226, 225, 224, 223, 222, 221, 220, 219, 218, 217, 216, 215, 214, 213, 212, 211, 210, 209, 208, 207, 206, 205, 204, 203, 202, 201, 200, 199, 198, 197, 196, 195, 194, 193, 192, 191, 190, 189, 188, 187, 186, 185, 184, 183, 182, 181, 180, 179, 178, 177, 176, 175, 174, 173, 172, 171, 170, 169, 168, 167, 166, 165, 164, 163, 162, 161, 160, 159, 158, 157, 156, 155, 154, 153, 152, 151, 150, 149, 148, 147, 146, 145, 144, 143, 142, 141, 140, 139, 138, 137, 136, 135, 134, 133, 132, 131, 130, 129, 128, 127, 126, 125, 124, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 103, 102, 101, 100, 99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 89, 88, 87, 86, 85, 84, 83, 82, 81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, Page 4186 was succesfully scraped.\n",
      "Page 4190 was succesfully scraped.\n",
      "Page 4200 was succesfully scraped.\n",
      "Page 4210 was succesfully scraped.\n",
      "Page 4220 was succesfully scraped.\n",
      "Page 4230 was succesfully scraped.\n",
      "Page 4240 was succesfully scraped.\n",
      "Page 4250 was succesfully scraped.\n",
      "Page 4260 was succesfully scraped.\n",
      "Page 4270 was succesfully scraped.\n",
      "Page 4280 was succesfully scraped.\n",
      "Page 4290 was succesfully scraped.\n",
      "Page 4300 was succesfully scraped.\n",
      "Page 4310 was succesfully scraped.\n",
      "Page 4320 was succesfully scraped.\n",
      "Page 4330 was succesfully scraped.\n",
      "Page 4340 was succesfully scraped.\n",
      "Page 4350 was succesfully scraped.\n",
      "Page 4360 was succesfully scraped.\n",
      "Page 4370 was succesfully scraped.\n",
      "Page 4380 was succesfully scraped.\n",
      "For some reason data scraping couldn't be executed for page 4383.\n",
      "Maybe the page we selected to scrape doesn't exist.\n",
      "Or we reached the amount of data we could scrape within in a specific time period.\n",
      "Let's wait a few minutes and start scraping again.\n",
      "============================================================\n",
      "Countdown 8 minutes :\n",
      "============================================================\n",
      "480, 479, 478, 477, 476, 475, 474, 473, 472, 471, 470, 469, 468, 467, 466, 465, 464, 463, 462, 461, 460, 459, 458, 457, 456, 455, 454, 453, 452, 451, 450, 449, 448, 447, 446, 445, 444, 443, 442, 441, 440, 439, 438, 437, 436, 435, 434, 433, 432, 431, 430, 429, 428, 427, 426, 425, 424, 423, 422, 421, 420, 419, 418, 417, 416, 415, 414, 413, 412, 411, 410, 409, 408, 407, 406, 405, 404, 403, 402, 401, 400, 399, 398, 397, 396, 395, 394, 393, 392, 391, 390, 389, 388, 387, 386, 385, 384, 383, 382, 381, 380, 379, 378, 377, 376, 375, 374, 373, 372, 371, 370, 369, 368, 367, 366, 365, 364, 363, 362, 361, 360, 359, 358, 357, 356, 355, 354, 353, 352, 351, 350, 349, 348, 347, 346, 345, 344, 343, 342, 341, 340, 339, 338, 337, 336, 335, 334, 333, 332, 331, 330, 329, 328, 327, 326, 325, 324, 323, 322, 321, 320, 319, 318, 317, 316, 315, 314, 313, 312, 311, 310, 309, 308, 307, 306, 305, 304, 303, 302, 301, 300, 299, 298, 297, 296, 295, 294, 293, 292, 291, 290, 289, 288, 287, 286, 285, 284, 283, 282, 281, 280, 279, 278, 277, 276, 275, 274, 273, 272, 271, 270, 269, 268, 267, 266, 265, 264, 263, 262, 261, 260, 259, 258, 257, 256, 255, 254, 253, 252, 251, 250, 249, 248, 247, 246, 245, 244, 243, 242, 241, 240, 239, 238, 237, 236, 235, 234, 233, 232, 231, 230, 229, 228, 227, 226, 225, 224, 223, 222, 221, 220, 219, 218, 217, 216, 215, 214, 213, 212, 211, 210, 209, 208, 207, 206, 205, 204, 203, 202, 201, 200, 199, 198, 197, 196, 195, 194, 193, 192, 191, 190, 189, 188, 187, 186, 185, 184, 183, 182, 181, 180, 179, 178, 177, 176, 175, 174, 173, 172, 171, 170, 169, 168, 167, 166, 165, 164, 163, 162, 161, 160, 159, 158, 157, 156, 155, 154, 153, 152, 151, 150, 149, 148, 147, 146, 145, 144, 143, 142, 141, 140, 139, 138, 137, 136, 135, 134, 133, 132, 131, 130, 129, 128, 127, 126, 125, 124, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 103, 102, 101, 100, 99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 89, 88, 87, 86, 85, 84, 83, 82, 81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, Page 4383 was succesfully scraped.\n",
      "Page 4390 was succesfully scraped.\n",
      "Page 4400 was succesfully scraped.\n",
      "Page 4410 was succesfully scraped.\n",
      "Page 4420 was succesfully scraped.\n",
      "Page 4430 was succesfully scraped.\n",
      "Page 4440 was succesfully scraped.\n",
      "Page 4450 was succesfully scraped.\n",
      "Page 4460 was succesfully scraped.\n",
      "Page 4470 was succesfully scraped.\n",
      "Page 4480 was succesfully scraped.\n",
      "Page 4490 was succesfully scraped.\n",
      "Page 4500 was succesfully scraped.\n",
      "Page 4510 was succesfully scraped.\n",
      "Page 4520 was succesfully scraped.\n",
      "Page 4530 was succesfully scraped.\n",
      "Page 4540 was succesfully scraped.\n",
      "Page 4550 was succesfully scraped.\n",
      "Page 4560 was succesfully scraped.\n",
      "For some reason data scraping couldn't be executed for page 4570.\n",
      "Maybe the page we selected to scrape doesn't exist.\n",
      "Or we reached the amount of data we could scrape within in a specific time period.\n",
      "Let's wait a few minutes and start scraping again.\n",
      "============================================================\n",
      "Countdown 8 minutes :\n",
      "============================================================\n",
      "480, 479, 478, 477, 476, 475, 474, 473, 472, 471, 470, 469, 468, 467, 466, 465, 464, 463, 462, 461, 460, 459, 458, 457, 456, 455, 454, 453, 452, 451, 450, 449, 448, 447, 446, 445, 444, 443, 442, 441, 440, 439, 438, 437, 436, 435, 434, 433, 432, 431, 430, 429, 428, 427, 426, 425, 424, 423, 422, 421, 420, 419, 418, 417, 416, 415, 414, 413, 412, 411, 410, 409, 408, 407, 406, 405, 404, 403, 402, 401, 400, 399, 398, 397, 396, 395, 394, 393, 392, 391, 390, 389, 388, 387, 386, 385, 384, 383, 382, 381, 380, 379, 378, 377, 376, 375, 374, 373, 372, 371, 370, 369, 368, 367, 366, 365, 364, 363, 362, 361, 360, 359, 358, 357, 356, 355, 354, 353, 352, 351, 350, 349, 348, 347, 346, 345, 344, 343, 342, 341, 340, 339, 338, 337, 336, 335, 334, 333, 332, 331, 330, 329, 328, 327, 326, 325, 324, 323, 322, 321, 320, 319, 318, 317, 316, 315, 314, 313, 312, 311, 310, 309, 308, 307, 306, 305, 304, 303, 302, 301, 300, 299, 298, 297, 296, 295, 294, 293, 292, 291, 290, 289, 288, 287, 286, 285, 284, 283, 282, 281, 280, 279, 278, 277, 276, 275, 274, 273, 272, 271, 270, 269, 268, 267, 266, 265, 264, 263, 262, 261, 260, 259, 258, 257, 256, 255, 254, 253, 252, 251, 250, 249, 248, 247, 246, 245, 244, 243, 242, 241, 240, 239, 238, 237, 236, 235, 234, 233, 232, 231, 230, 229, 228, 227, 226, 225, 224, 223, 222, 221, 220, 219, 218, 217, 216, 215, 214, 213, 212, 211, 210, 209, 208, 207, 206, 205, 204, 203, 202, 201, 200, 199, 198, 197, 196, 195, 194, 193, 192, 191, 190, 189, 188, 187, 186, 185, 184, 183, 182, 181, 180, 179, 178, 177, 176, 175, 174, 173, 172, 171, 170, 169, 168, 167, 166, 165, 164, 163, 162, 161, 160, 159, 158, 157, 156, 155, 154, 153, 152, 151, 150, 149, 148, 147, 146, 145, 144, 143, 142, 141, 140, 139, 138, 137, 136, 135, 134, 133, 132, 131, 130, 129, 128, 127, 126, 125, 124, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 103, 102, 101, 100, 99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 89, 88, 87, 86, 85, 84, 83, 82, 81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, Page 4570 was succesfully scraped.\n",
      "Page 4580 was succesfully scraped.\n"
     ]
    }
   ],
   "source": [
    "df5 = get_specific_data_for_specific_pages(first_page = 4000, last_page = 4590) # careful about the value of last_page\n",
    "df5.to_csv('asurion5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c702641c",
   "metadata": {},
   "source": [
    "# `WARNING`:\n",
    "\n",
    "If we put a higher number for last_page than how many actually exist, the program will run infinitely and nothing will be added to the variable df5.\n",
    "\n",
    "This part should be still handled so the program automatically stops when we reach the last page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66425881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.concat([df1,df2,df3,df4,df5])\\\n",
    "#    .reset_index(drop=True) \\\n",
    "#    .drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8f627a",
   "metadata": {},
   "source": [
    "Scraping all the data was't done all at once so we got the data with 1000 page batches.\n",
    "\n",
    "We import these into a final df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69ac4a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91773, 8)\n"
     ]
    }
   ],
   "source": [
    "# import all files that has the scraped data\n",
    "data_files = ['asurion1.csv', 'asurion2.csv', 'asurion3.csv', 'asurion4.csv', 'asurion5.csv']\n",
    "df = pd.concat((pd.read_csv(filename) for filename in data_files))\n",
    "df = df.drop_duplicates() \\\n",
    "    .reset_index(drop = True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88a09de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titre</th>\n",
       "      <th>nom</th>\n",
       "      <th>stars</th>\n",
       "      <th>localisation</th>\n",
       "      <th>nb_reviews</th>\n",
       "      <th>date_review</th>\n",
       "      <th>date_experience</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fast efficient and simple process for repair</td>\n",
       "      <td>Debbie Denmark</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>2023-05-24T19:47:54.000Z</td>\n",
       "      <td>Date of experience: May 20, 2023</td>\n",
       "      <td>My husband cracked his iPhone screen on a Satu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A+ service</td>\n",
       "      <td>Brandon</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>2023-05-24T21:56:18.000Z</td>\n",
       "      <td>Date of experience: May 24, 2023</td>\n",
       "      <td>I was having difficulty with a tablet being se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Excellent service</td>\n",
       "      <td>Kevin P.</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>2023-05-24T20:18:21.000Z</td>\n",
       "      <td>Date of experience: May 18, 2023</td>\n",
       "      <td>With any insurance claim you obviously have to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I was out fishing on the lake another…</td>\n",
       "      <td>John Frazzini</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>US</td>\n",
       "      <td>3 reviews</td>\n",
       "      <td>2023-05-24T18:38:46.000Z</td>\n",
       "      <td>Date of experience: May 19, 2023</td>\n",
       "      <td>I was out fishing on the lake another boat sid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Everything went very well</td>\n",
       "      <td>JOSEPHINE</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>2023-05-24T17:02:02.000Z</td>\n",
       "      <td>Date of experience: May 20, 2023</td>\n",
       "      <td>Everything went very well except I was confuse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91768</th>\n",
       "      <td>Verizon replacement</td>\n",
       "      <td>RUTH P</td>\n",
       "      <td>Rated 3 out of 5 stars</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>2016-10-24T22:37:03.000Z</td>\n",
       "      <td>Date of experience: October 24, 2016</td>\n",
       "      <td>Prongs were bent on first phone.  I had to pay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91769</th>\n",
       "      <td>Replacement cost</td>\n",
       "      <td>THOMAS D</td>\n",
       "      <td>Rated 4 out of 5 stars</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>2016-10-24T21:49:43.000Z</td>\n",
       "      <td>Date of experience: October 24, 2016</td>\n",
       "      <td>Then cost of a rebuilt phone is high, consider...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91770</th>\n",
       "      <td>Great</td>\n",
       "      <td>PEGGY</td>\n",
       "      <td>Rated 4 out of 5 stars</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>2016-10-24T21:20:12.000Z</td>\n",
       "      <td>Date of experience: October 24, 2016</td>\n",
       "      <td>Good customer service! Thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91771</th>\n",
       "      <td>As always - an easy transaction</td>\n",
       "      <td>Casey S.</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>2016-10-24T20:35:26.000Z</td>\n",
       "      <td>Date of experience: October 24, 2016</td>\n",
       "      <td>I have teenage boys who are not careful with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91772</th>\n",
       "      <td>Great Job</td>\n",
       "      <td>MATT</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>2016-10-24T19:39:53.000Z</td>\n",
       "      <td>Date of experience: October 24, 2016</td>\n",
       "      <td>Asurion sent my new phone quickly.  Thank you ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91773 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              titre             nom  \\\n",
       "0      Fast efficient and simple process for repair  Debbie Denmark   \n",
       "1                                        A+ service         Brandon   \n",
       "2                                 Excellent service        Kevin P.   \n",
       "3            I was out fishing on the lake another…   John Frazzini   \n",
       "4                         Everything went very well       JOSEPHINE   \n",
       "...                                             ...             ...   \n",
       "91768                           Verizon replacement          RUTH P   \n",
       "91769                              Replacement cost        THOMAS D   \n",
       "91770                                         Great          PEGGY    \n",
       "91771               As always - an easy transaction        Casey S.   \n",
       "91772                                     Great Job            MATT   \n",
       "\n",
       "                        stars localisation nb_reviews  \\\n",
       "0      Rated 5 out of 5 stars           US   1 review   \n",
       "1      Rated 5 out of 5 stars           US   1 review   \n",
       "2      Rated 5 out of 5 stars           US   1 review   \n",
       "3      Rated 5 out of 5 stars           US  3 reviews   \n",
       "4      Rated 5 out of 5 stars           US   1 review   \n",
       "...                       ...          ...        ...   \n",
       "91768  Rated 3 out of 5 stars           US   1 review   \n",
       "91769  Rated 4 out of 5 stars           US   1 review   \n",
       "91770  Rated 4 out of 5 stars           US   1 review   \n",
       "91771  Rated 5 out of 5 stars           US   1 review   \n",
       "91772  Rated 5 out of 5 stars           US   1 review   \n",
       "\n",
       "                    date_review                       date_experience  \\\n",
       "0      2023-05-24T19:47:54.000Z      Date of experience: May 20, 2023   \n",
       "1      2023-05-24T21:56:18.000Z      Date of experience: May 24, 2023   \n",
       "2      2023-05-24T20:18:21.000Z      Date of experience: May 18, 2023   \n",
       "3      2023-05-24T18:38:46.000Z      Date of experience: May 19, 2023   \n",
       "4      2023-05-24T17:02:02.000Z      Date of experience: May 20, 2023   \n",
       "...                         ...                                   ...   \n",
       "91768  2016-10-24T22:37:03.000Z  Date of experience: October 24, 2016   \n",
       "91769  2016-10-24T21:49:43.000Z  Date of experience: October 24, 2016   \n",
       "91770  2016-10-24T21:20:12.000Z  Date of experience: October 24, 2016   \n",
       "91771  2016-10-24T20:35:26.000Z  Date of experience: October 24, 2016   \n",
       "91772  2016-10-24T19:39:53.000Z  Date of experience: October 24, 2016   \n",
       "\n",
       "                                                 comment  \n",
       "0      My husband cracked his iPhone screen on a Satu...  \n",
       "1      I was having difficulty with a tablet being se...  \n",
       "2      With any insurance claim you obviously have to...  \n",
       "3      I was out fishing on the lake another boat sid...  \n",
       "4      Everything went very well except I was confuse...  \n",
       "...                                                  ...  \n",
       "91768  Prongs were bent on first phone.  I had to pay...  \n",
       "91769  Then cost of a rebuilt phone is high, consider...  \n",
       "91770                      Good customer service! Thanks  \n",
       "91771  I have teenage boys who are not careful with t...  \n",
       "91772  Asurion sent my new phone quickly.  Thank you ...  \n",
       "\n",
       "[91773 rows x 8 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "178fda1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the files into one CSV file\n",
    "df.to_csv('asurion_complete.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "533bd9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the files into one JSON file\n",
    "df.to_json('asurion_complete.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
