{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def scraper_functions(urls):\n",
    "    response = requests.get(urls)\n",
    "    if response.status_code == 200:\n",
    "        soup = bs(response.content,'html.parser')\n",
    "        pattern = r'\\d+\\.\\d+'\n",
    "        pattern1 = r'\\d+'\n",
    "\n",
    "        # Créer des listes vides pour stocker les informations\n",
    "        noms_entreprises = []\n",
    "        notes = []\n",
    "        nombres_avis = []\n",
    "        pourcentages_avis_excellents = []\n",
    "        domaines = []\n",
    "\n",
    "        # Trouver la balise qui contient toutes les entreprises\n",
    "        entreprises = soup.find_all('div', {'class': 'paper_paper__1PY90 paper_outline__lwsUX card_card__lQWDv card_noPadding__D8PcU styles_wrapper__2JOo2'})\n",
    "\n",
    "        # Parcourir les entreprises et extraire les informations pertinentes\n",
    "        for entreprise in entreprises:\n",
    "            noms = entreprise.find('div',attrs={'class':'styles_businessUnitMain__PuwB7'}).find('p', attrs={'class': 'styles_displayName__GOhL2'}).text\n",
    "            note_Span = entreprise.find('div', attrs={'class': 'styles_businessUnitMain__PuwB7'}).find('span', attrs={'class': 'styles_trustScore__8emxJ'})\n",
    "            note = note_Span.text if (note_Span)is not None else ''\n",
    "            match = re.search(pattern, note)\n",
    "            if match:\n",
    "                note = match.group(0)\n",
    "            nombre = entreprise.find('div',attrs={'class':'styles_businessUnitMain__PuwB7'}).find('p', attrs={'class': 'styles_ratingText__yQ5S7'})\n",
    "            nombre = nombre.text.replace('\\n','').split('|')[-1].strip() if nombre is not None else ''\n",
    "            match = re.search(pattern1, nombre)\n",
    "            if match:\n",
    "                nombre = match.group(0)\n",
    "            span = entreprise.find('div',attrs={'class':'styles_businessUnitMain__PuwB7'}).find('div',attrs={'class':'star-rating_starRating__4rrcf star-rating_responsive__C9oka'})\n",
    "            pourcentage = span.find('img')['alt'] if span is not None else ''\n",
    "\n",
    "            domaine_span_list = entreprise.find('div', attrs={'class': 'styles_wrapper___E6__ styles_categoriesLabels__FiWQ4 styles_desktop__U5iWw'}).find_all('span')\n",
    "            domaine = domaine_span_list[2].text.strip() if len(domaine_span_list) > 2 else ''\n",
    "\n",
    "            # Ajouter les informations à leurs listes respectives\n",
    "            noms_entreprises.append(noms)\n",
    "            notes.append(note)\n",
    "            nombres_avis.append(nombre.replace(',', ''))\n",
    "            pourcentages_avis_excellents.append(pourcentage)\n",
    "            domaines.append(domaine)\n",
    "\n",
    "        # Créer un DataFrame pandas avec les informations\n",
    "        df = pd.DataFrame({'Nom':noms_entreprises, 'Note':notes, 'Nombre d\\'avis':nombres_avis, 'Pourcentage avis excellents':pourcentages_avis_excellents, 'Domaine':domaines})\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"La page {urls} n'a pas pu être récupérée (code de réponse HTTP {response.status_code})\")\n",
    "        return None\n",
    "\n",
    "def get_all_pages_categorie():\n",
    "    categories = ['cashback_provider','bank','atm','check_cashing_service','cryptocurrency_service','currency_exchange_service','financial_institution','money_transfer_service','money_order_service','savings_bank','trust_bank','escrow_service']\n",
    "    # Nombre maximal de pages à scraper pour chaque catégorie\n",
    "    max_pages_per_category = [3, 7, 1, 1, 55, 9, 3, 7, 1, 1, 1, 2]\n",
    "\n",
    "    # Scraper toutes les pages de chaque catégorie \n",
    "    urls = []  \n",
    "    for i, category in enumerate(categories):\n",
    "        max_pages = max_pages_per_category[i]\n",
    "        for page_number in range(max_pages):\n",
    "            page_url = f\"https://www.trustpilot.com/categories/{category}?page={page_number+1}\"\n",
    "            urls.append(page_url)\n",
    "    return urls    \n",
    "\n",
    "# Scrapping all pages \n",
    "def scraping_all_page():\n",
    "    pages =  get_all_pages_categorie()\n",
    "    all_data = []\n",
    "    for page in pages:\n",
    "        df = scraper_functions(urls=page)\n",
    "        all_data.append(df)\n",
    "    result = pd.concat(all_data, ignore_index=True)\n",
    "    return result\n",
    "\n",
    "# Exporter les données dans un fichier CSV\n",
    "data = scraping_all_page()\n",
    "data.to_csv('trustpilot_data.csv', sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
